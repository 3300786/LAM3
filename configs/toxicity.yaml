provider: perspective          # perspective | openai | detoxify | hf | llamaguard3
threshold: 1                   # 0-100，>= 阈值视为 toxic
batch_size: 16
max_retries: 5
timeout_s: 5
rate_limit:
  perspective_qps: 0.5
  openai_rps: 10
hf:
  model_name: "unitary/toxic-bert"
detoxify:
  variant: "original"       # original / unbiased / multilingual
  checkpoint: "/data2/wangjingsong/.cache/torch/hub/checkpoints/toxic_original-c1212f89.ckpt"
openai:
  categories: ["hate","harassment","self-harm","violence","sexual","illicit"]
perspective:
  attributes: ["TOXICITY","SEVERE_TOXICITY","INSULT","PROFANITY","THREAT"]
llamaguard3:
  model_id: "LLM-Research/Llama-Guard-3-8B"
  device: "cuda"
  dtype: "bf16"