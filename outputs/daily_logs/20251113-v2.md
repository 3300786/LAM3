
---

# 2025-11-13 LAM3 Daily Experiment Log

## 1. Overview

* Responsible: **JsW**
* Branch: **feat/feasibility**
* Models：**LLaVA-1.5-7B**, **IDEFICS2-8B**
* Evaluation Modules：Detoxify（可用）、Perspective（已获得 API Key）、OpenAI Moderation（功能受限）
* Current Focus：毒性评测能力增强；为 policy-risk 检测引入新模型；完善 MLLM 抽象层
* Dataset：`data/mini_bench`
* 今日目标：

  1. 搭建毒性评测的多源模型体系；
  2. 对比“社交毒性评测”与“政策风险评测”的能力差异；
  3. 确立 server 侧运行 + 本地评测 的模式；
  4. 规划 Llama-guard-2-8B 与其他小模型加入评测体系。

---

## 2. Parameters & Settings

### 2.1 Runtime（生成配置）

| Category     | Key              | Value | Notes                 |
| ------------ | ---------------- | ----- | --------------------- |
| Generation   | max_new_tokens   | 256   | 统一上限                  |
| Generation   | min_new_tokens   | 64    | 未在 wrapper 中使用        |
| Sampling     | do_sample        | true  | LLaVA wrapper 仍强制关闭采样 |
| Sampling     | temperature      | 0.2   | 采样关闭时不生效              |
| Sampling     | top_p            | 0.95  | 同上                    |
| Seed         | seed             | 42    | 全局统一                  |
| Precision    | precision        | bf16  | 非量化分支适用               |
| Quantization | enabled          | true  | 默认开启 4bit             |
| Quantization | compute_dtype    | bf16  | BitsAndBytes          |
| Quantization | quant_type       | nf4   | 默认                    |
| Quantization | use_double_quant | true  | 默认                    |

### 2.2 Toxicity / Risk Evaluation（评测配置）

模块：`src/metrics/toxicity.py`、`src/metrics/toxicity_providers.py`

可用 provider 状态：

| Provider                | 状态                    | 说明                              |
| ----------------------- | --------------------- | ------------------------------- |
| Detoxify                | **Fully Working**     | 本地推理，适合社交 toxicity              |
| Perspective API         | **API Key 获取成功，但需外网** | 暂时需要“server 生成 → 下载本地 → 本地调用”模式 |
| OpenAI Moderation       | **功能受限**              | 免费额度无法高频；不适合大规模实验               |
| HF pipeline（toxic-bert） | 可用                    | CPU 推理，性能较差                     |

新增规划模型：

| 目标             | 模型                        | 说明                        |
| -------------- | ------------------------- | ------------------------- |
| Policy-risk 检测 | **Llama-guard-2-8B**      | 用于识别“爆炸、暴力、犯罪”等 policy 风险 |
| Zero-shot 风险判定 | 小型 RoBERTa / DeBERTa 安全模型 | 用于辅助风险交叉验证                |

---

## 3. Observations / Results

### （1）毒性 / 风险评测体系的主要观察

1. **Perspective API Key 已成功获取**

   * 但 server 无外网 → 目前无法直接调用。
   * **拟采取流程：**
     **Server 生成 response → 下载本地 → 本地运行 Perspective API 评测 → 上传结果**。

2. **OpenAI Moderation API 在本实验中不可作为主评测**

   * 免费额度限制严格，无法支持大量请求。
   * 因此更适合作为“少量参考”而非主指标。

3. **Detoxify + Perspective 对社交 toxicity 识别能力强**

   * 对仇恨言论、侮辱、暴力言辞评测表现良好。
   * 但对 policy-risk（犯罪/爆炸/武器/违法等）**评分极低，几乎不触发**。
   * 说明它们属于“社交毒性模型”，不是“安全风险模型”。

4. **政策风险评测缺口明显，需要新模型补齐**
   目前 LAM3 评测体系尚缺少对以下内容的有效识别：

   * 越狱攻击常见目标：制造炸药、武器制作、诈骗、毒品、黑客攻击
   * 极其关键：社交 toxicity ≠ 安全违规（policy violation）

### （2）新模型规划

1. **Llama-guard-2-8B**

   * 专门用于“安全边界判定（policy risk）”
   * 对 Qwen、LLaMA3、Kimi 等大模型生态均常用
   * 可作为 **risk classifier**，并输出 policy-level 标签（如 self-harm, violence, illegal activities）

**将作为 LAM3 的主要“policy-risk 评测模型”。**

2. **加入小参数 zero-shot 模型进行辅助评测**
   包括：

   * `facebook/roberta-large-mnli`（适合分类）
   * 安全向的小模型（`Meta-Llama-Guard` 小参数版、`hf-security` 系列模型）
   * 有助于：

     * 交叉验证 policy-risk
     * 作为 Detoxify 的补充
     * 支持脱离大模型的独立评测流程

---

## 4. Issues & Bug Tracking

| ID          | Description                                                | Severity | Status            | Note                                      |
| ----------- | ---------------------------------------------------------- | -------- | ----------------- | ----------------------------------------- |
| B20251113-1 | Perspective / OpenAI Moderation 均需外网，本服务器无法直接调用            | High     | Open              | 将采用“远程生成、本地评测”模式                          |
| B20251113-2 | OpenAI Moderation 免费额度限制，无法用于大规模实验                         | High     | Closed-as-WontFix | 改用 Perspective + Llama-guard              |
| B20251113-3 | Detoxify 与 Perspective 对 policy-risk 几乎无感知                 | High     | **New**           | 需新增 Llama-guard-2-8B 作为 risk classifier   |
| B20251113-4 | 未建立“社交 toxicity + policy-risk”双指标体系                        | Medium   | **New**           | 需要拆分 Toxicity→Social Toxicity；Policy Risk |
| B20251113-5 | 评测模块目前只支持“单文本→单判断”，未包含多标签合并策略                              | Medium   | Open              | 后续需添加 ensemble 机制                         |
| B20251113-6 | 服务器-本地的两阶段评测流程未自动化                                         | Medium   | New               | 需增加导出 response → local eval 的脚本           |
| B20251113-7 | Llama-guard-2-8B 尚未加入评测模块（缺 wrapper）                       | Medium   | New               | 需在 `src/metrics/risk_providers.py` 中新增    |
| B20251112-系 | （昨日未解决的问题仍保留）如 revision 透传、local_files_only、device_map 控制等 | Mixed    | Open              | 需在后续统一处理                                  |

---

## 5. Next Actions

### **短期（1–2 天）**

* 将 Perspective API 与 Detoxify 统一写入：

  * `configs/toxicity_detoxify.yaml`
  * `configs/toxicity_perspective.yaml`
* 设计 server→local 的双阶段评测流程：

  ```
  server: 生成 response → 导出 response.jsonl
  local: 加载 response.jsonl → 调 perspective → 写出 toxicity_scored.jsonl
  ```
* 启动 Llama-guard-2-8B 的 wrapper 实现（用于 zero-shot risk detection）。

### **中期（3–5 天）**

* 建立“双指标体系”：
  **Social Toxicity（Detoxify/Perspective） + Policy Risk（Llama-guard）**
* 在 `metrics/` 下实现 risk provider：

  * `LlamaGuardClient`
  * `ZeroShotRiskClassifier`

### **长期（下周）**

* 将多个评测源进行“多裁判融合”（ensemble）

  * 规则加权
  * 温和投票
  * 不一致时的仲裁机制
* 完整接入 cross-attention-gradient 攻击，并做 ASR/Toxicity/Risk 综合评测。

---

## 6. Commit & Sync

* Branch: `feat/feasibility`
* Git Commit: *待补充（按实际 HEAD）*
* Log Path: `outputs/daily_logs/20251113.md`
* Timestamp: 2025-11-13 JST

---
